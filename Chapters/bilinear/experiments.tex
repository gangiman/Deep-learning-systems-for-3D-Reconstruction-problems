\section{Experiments}




\indent\textbf{Datasets and evaluation protocols}. \\
We investigate the performance of the CNN method and its Bilinear variant (figure \ref{fig:architecture}) for three re-identification datasets: CUHK01 \citep{LiZW12}, CUHK03 \citep{Li14} and Market-1501 \citep{zheng2015scalable}. Following \citep{Li14}, we use Recall@K metric to report our results.



\begin{table}%{l}%{8cm}
\begin{center}
% # discr null space {zhang2016learning}          62.55 90.05 94.80 98.10
% # me      {paisitkriangkrai2015learning}                  62.10 89.10 94.30 97.80
% # LOMO XQDA {liao2015person}                52.20 82.23 92.14 96.25
% # ahmed {ahmed2015improved}                    54.74 86.50 93.88 98.10
% # FPNN   {Li14}                   20.65 51.50 66.50 80.00
\caption{Recall@K for the CUHK03-labeled dataset.}
\begin{tabular}{c|ccccc}

\hline
Method                & r = 1   &  r = 5   & r = 10   & r = 20 \\
\hline
FPNN \citep{Li14}  & 20.65  & 51.50    & 66.50      & 80.00   \\
LOMO+XQDA \citep{liao2015person}  & 52.20  & 82.23    & 92.14         & 96.25   \\
ImrovedDeep \citep{ahmed2015improved}& 54.74 & 86.50    & 93.88      & 98.10   \\
ME \citep{paisitkriangkrai2015learning}
                       &62.10    & 89.10    &  94.30      & 97.80  \\
DiscrNullSpace \citep{zhang2016learning}&62.55  &   90.05  & 94.80        & 98.10  \\  
\hline
 CNN                   &64.15   & 91.66    & 96.97     &99.26   \\
 MR B-CNN      & \bf{69.7}    & \bf{93.37}    &\bf{98.91}    &\bf{99.39}   \\
 \hline
\end{tabular}
\label{tab:cuhk03_labeled}
\end{center}
\end{table} 



\begin{table}%{l}%{8cm}
\begin{center}
% gated {VariorHW16}    61.8 80.9  88.3
% # discr null space {zhang2016learning} 54.70 84.75 94.80 95.20
% # ahmed {ahmed2015improved}       44.96 76.01 83.47 93.15 
% # LOMO XQDA    {liao2015person}46.25 78.90 88.55 94.25
% # FPNN  {Li14}       19.89 50.00 64.00 78.50 
% # lstm  {VariorSLXW16}                        57.3 80.1 88.3
\caption{Recall@K for the CUHK03-detected dataset. The new architecture (MR B-CNN) outperforms other methods.}
\begin{tabular}{c|ccccc}
\hline
Method                & r = 1   &  r = 5   & r = 10   & r = 20 \\
\hline
 FPNN \citep{Li14}  & 19.89  & 50.00 & 64.00         & 78.50   \\
 ImrovedDeep \citep{ahmed2015improved}& 44.96 & 76.01  & 83.47        & 93.15   \\
 LOMO+XQDA \citep{liao2015person}   & 46.25 & 78.90  & 88.55       & 94.25   \\
 DiscrNullSpace \citep{zhang2016learning}& 54.70 & 84.75   & \bf{94.80}         & 95.20 \\
 SiamLSTM \citep{VariorSLXW16}    & 57.3   &80.1    & 88.3        &-        \\
 GatedSiamCNN \citep{VariorHW16}     & 61.8    &  80.9    & 88.3         & -      \\
 \hline
 CNN                   &58.09   & 87.06    & 93.38     & 97.17  \\
 MR B-CNN        &\bf{63.67}   &\bf{89.15}    & 94.66     & \bf{97.5}   \\
 \hline
\end{tabular}
\label{tab:cuhk03_detected}
\end{center}
\end{table} 

%%%%%%to main intro ------------------------------
% The CUHK01 dataset contains images of 971 identities from two disjoint camera views. Each identity has two samples per camera view. We used 485 randomly chosen identities for train and the other 486 for test.  

% The CUHK03 dataset includes 13,164 images of 1,360 pedestrians captured from 3 pairs of cameras. The two versions of the dataset are provided: \textit{CUHK03-labeled} and \textit{CUHK03-detected} with manually labeled bounding boxes and automatically detected ones accordingly. We provide results for both versions.


% In more detail, the evaluation protocol accepted for the CUHK03 is the following: 1,360 identities are split into 1,160 identities for training, 100 for validation and 100 for testing. At test time single-shot Recall@K curves are calculated. Five random splits are used for both CUHK01 and CUHK03 to calculate the resulting average Recall@K. Some sample images of CUHK03 dataset are shown in figure \ref{fig:teaser}.


% We also report our results on the Market-1501 dataset, introduced in \citep{zheng2015scalable}.
% %(See figure \ref{fig:datasets_market} for sample images). 
% This dataset contains 32,643 images of 1,501 identities, each identity is captured by from two to six cameras. The dataset is randomly divided into the test set of 750 identities and the train set of 751 identities. %For each identity in the test set one image in each camera is selected and used as a query. Manually drawn bounding boxes are used for query images, and automatically detected ones for training images as well as for the gallery images in the test set. 
% %Market-1501 also includes ''distractor'' images that correspond to false detections, which makes evaluation on the dataset even more realistic and challenging. %As in \citep{zheng2015scalable}, we also evaluate the re-identification performance using the multi-shot protocol (multiple images from the same camera are present for each identity in the gallery set) for both datasets. %ollowing \citep{zheng2015scalable}, we also demonstrate results with the multi-query approach, when all images of a person from one camera are used as a query, instead of using only one randomly chosen image as in the standard and the multi-shot protocols. 
% %As in \citep{zheng2015scalable} we investigate pooling methods for descriptors of query images for CUHK03 and Market-1501.
% %Although descriptors are merged only for images of a person from one camera, they still capture different poses, thus using pooled query descriptor might be better for re-identification performance.

% %Matching is done across different cameras, and in the situation when the gallery images of the same person and form the same camera as the query are ignored and not counted as true positives during the evaluation.
%------------------------------------------------------ 

\indent\textbf{Architectures.}\\
In the experiments, we compare the baseline CNN architecture of \citep{yi2014deep} as one of the baselines (it is described in \sect{intro_architectures}).
We also evaluate the baseline Bilinear CNN (\textit{''B-CNN''}) architecture where bilinear features are pooled over all locations for each of the three image parts. This corresponds to the formula (\ref{eq:desc}), where whole image is used for pooling.
Finally, we present the results for the Multi-region Bilinear CNN (\textit{''MR B-CNN''}) introduced in this paper (figure \ref{fig:architecture}).

\indent\textbf{Implementation details.} \\
To learn the embeddings, we use the Histogram loss introduced in \chapt{hist}.
As in \citep{yi2014deep}, we form training pairs inside each batch consisting of 128 randomly chosen training images (from all cameras). The training set is shuffled after each epoch, so the network can see many different image pairs while training. All images are resized to height $160$ and width $60$ pixels. Cosine similarity is used to compute the distance between a pair of image descriptors. 



\begin{table}
\begin{center}
% # gated siamese  {VariorHW16}  65.88  map 39.55
% # discr null space {zhang2016learning}             61.02  map 35.68      | MQ 71.56 map 46.03
% # lstm      {VariorSLXW16}               61.60            map 35.31
% # deep attr driven{SuZX0T16}          39.4 map 19.6
\caption{Recall@K for the Market-1501 dataset. The proposed architecture (MR B-CNN) outperforms other methods.}
\begin{tabular}{c|cccc}
\hline
Method                    & r = 1 & r = 5 & r = 10  &  mAP  \\
\hline
  DeepAttrDriven \citep{SuZX0T16}         & 39.4     & - & - & 19.6  \\
  DiscrNullSpace \citep{zhang2016learning}&  61.02   & - & - & 35.68 \\
  SiamLSTM \citep{VariorSLXW16}     &  61.60   & - & - & 35.31 \\
  GatedSiamCNN \citep{VariorHW16}       & 65.88  & - & - & 39.55 \\
 \hline
 CNN                  & 56.62 &  78.92 &  85.15&   32.97 \\ %& 78.92 & 85.15 & 88.27 & 89.79 \\
 MR B-CNN      & \bf{66.36} & \bf{85.01} & \bf{90.17}  & \bf{41.17} \\%& 85.01 & 90.17 & 92.76 & 94.09   \\
 \hline
\end{tabular}
\label{tab:market}
\end{center}
\end{table} 

% 0.5662114   0.6716152   0.72713777  0.76425178  0.7891924
%   0.80849169  0.82214964  0.8358076   0.84590261  0.85154394  0.85956057
%   0.86579572  0.87351544  0.87767221  0.88271971  0.88568884  0.88984561
%   0.8922209   0.89519002  0.89786223  0.90172209  0.90528504  0.90587886
%   0.90706651  0.91003563  0.91211401  0.91389549  0.91686461  0.9192399
%   0.92131829  0.92339667  0.92547506  0.92755344  0.92874109  0.92933492
%   0.93081948  0.93230404  0.93408551  0.93557007  0.93675772  0.93824228
%   0.93913302  0.93942993  0.94091449  0.9412114   0.94239905  0.94328979
%   0.94418052  0.94447743  0.94566508
  

%0.66359857  0.75593824  0.79928741  0.82808789  0.85005938
%   0.8652019   0.87826603  0.88747031  0.89519002  0.90172209  0.90944181
%   0.91567696  0.91894299  0.92250594  0.92755344  0.93111639  0.93408551
%   0.93675772  0.93913302  0.94091449  0.9435867   0.94566508  0.94714964
%   0.9486342   0.95041568  0.95160333  0.95190024  0.95279097  0.95368171
%   0.95427553  0.95546318  0.95694774  0.95813539  0.95902613  0.96021378
%   0.96140143  0.96258907  0.96377672  0.96407363  0.96437055  0.96466746
%   0.96496437  0.96585511  0.96585511  0.96674584  0.96674584  0.96704276
%   0.96733967  0.96793349  0.96793349



\begin{table}%{l}%{8cm}
\begin{center}
\caption{Recall@K for the CUHK01 dataset. For CNN and MR B-CNN,  single-shot protocol with 486 queries was used. We include some results for this dataset, although we are not sure which protocol is used in \citep{zhang2016learning}. Other works use the same protocol as ours.}
\begin{tabular}{c|ccccc}
\hline
Method                & r = 1   &  r = 5   & r = 10   & r = 20 \\
\hline
ImrovedDeep \citep{ahmed2015improved} &47.53& 71.60& 80.25& 87.45\\
ME \citep{paisitkriangkrai2015learning} & 53.40 & 76.40 & 84.40 & 90.50\\
DiscrNullSpace \citep{zhang2016learning} & 69.09 &86.87 &91.77& 95.39\\
 \hline
CNN     & 48.04 &74.34 &83.33& 90.48 \\
MR B-CNN & 52.88 &78.08& 86.3& 92.63 \\
 \hline
\end{tabular}
\label{tab:cuhk01}
\end{center}
\end{table} 


We train networks with the weight decay rate of $0.0005$. The learning rate is changing according to the ``step'' policy, the initial learning rate is set to $10^{-4}$ and it is divided by ten when the performance on the validation set stops improving (which is roughly every $100,000$ iterations). 
The dropout layer with probability of 0.5 is inserted before the fully connected layer. The best iteration is chosen using the validation set. Following \citep{ahmed2015improved}, for CUHK01 we finetune the net pretrained on CUHK03. 


\indent\textbf{Variations of the Bilinear CNN architecture.} \\
We have conducted a number of experiments with the varying pooling area for bilinear features (MR B-CNN), including full area pooling (B-CNN), on the CUHK03-labeled.  Here we demonstrate results for our current MR B-CNN architecture with $5\times5$ pooling area, as this architecture has been found to be the most beneficial for the CUHK03 dataset.
We also compare results for B-CNN architecture, where no spatial information is preserved. In \fig{recall}a and \fig{recall}b B-CNN is shown to be outperformed by other two architectures by a large margin. This result is not specific to a particular loss, as we observed the same in our preliminary experiments with the Binomial Deviance loss \eq{bindev} \citep{yi2014deep}. Interestingly, in our previous experiments with the Binomial Deviance loss, the simple B-CNN variant slightly outperformed baseline CNN architecture on  CUHK03-detected.
The MR B-CNN architecture shows uniform improvement over baseline CNN architecture on all three datasets (\fig{recall}a,b,c,d).
%During previous experimentation, we have also made sure that having 2 separate sub-networks for feature extraction is important for the bilinear architecture for the re-id task. 

%These experiments were performed using the original Binomial Deviance\citep{yi2014deep} loss function for learning the model.



\indent\textbf{Comparison with the state-of-the-art methods.} \\
To our knowledge, Multi-region Bilinear CNN networks introduced in this paper outperform previously published methods on the CUHK03 (both 'detected' and 'labeled' versions), and Market-1501 datasets. Recall@K for several rank values are shown in \tab{cuhk03_labeled}, \tab{cuhk03_detected} and \tab{market} (singe query setting was used). For the Market-1501 dataset, mean average precision value is additionally shown. The results for CUHK01 are shown in \tab{cuhk01}. 

%Here, we only include methods that map images to descriptors that can be compared using simple metrics. Some of the methods that perform pairwise comparisons using deep networks that need to be evaluated for all compared pairs of descriptors achieve higher re-identification accuracy, however do not scale for large-scale systems. 





