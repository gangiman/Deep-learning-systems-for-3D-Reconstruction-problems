\section{Related work}

\textbf{Deep CNNs for Re-Identifications.} Several CNN-based methods for person re-identification have been proposed recently\cite{li2014deepreid, yi2014deep, ahmed2015improved, chen2015deep, VariorHW16, VariorSLXW16,SuZX0T16, LiuFQJY17, XiaoLOW16}. Yi~\etal~\cite{yi2014deep} were among the first to evaluate ``siamese'' architectures that accomplishes embedding of pedestrian images into the descriptor space, where they can be further compared using cosine distance. In \cite{yi2014deep}, a peculiar architecture specific to pedestrian images is proposed that includes three independent sub-networks corresponding to three regions (legs, torso, head-and-shoulders). This is done in order to take into account the variability of the statistics of textures, shapes, and articulations between the three regions. Our architecture includes the network of  Yi~\etal~\cite{yi2014deep} as its part.

Apart from \cite{yi2014deep}, \cite{li2014deepreid} and \cite{ahmed2015improved} learn classification networks that can categorize a pair of images as either depicting  the same subjects or different subjects. %In both approaches, the two images are fed into the network, and the difference in their mid-level representation is processed by additional special layers (Patch matching in \cite{li2014deepreid} and cross-input neighbourhood difference in \cite{ahmed2015improved}).% After that, extra convolutional and inner product layers are applied to the combined representation of the pair of images. Finally, the classification for the pair of images is performed using softmax layer. 
The proposed deep learning approaches \cite{ahmed2015improved, yi2014deep, li2014deepreid}, while competitive, do not clearly outperform more traditional approaches based on ``hand-engineered'' features \cite{paisitkriangkrai2015learning, zhao2014person}.

Unfortunately, when searching for matches in a dataset, the methods proposed in \cite{li2014deepreid}, \cite{ahmed2015improved} and \cite{chen2015deep} need to process pairs that include the query and every image in the dataset, and hence cannot directly utilize fast retrieval methods based on Euclidean and other simple distances. Here we aim at the approach that can learn per-image descriptors and then compare them with cosine similarity measure. This justifies starting with the architecture proposed in  \cite{yi2014deep} and then modifying it by inserting new layers.

There are several new works reporting results that are better than ours \cite{LiuFQJY17, XiaoLOW16} where additional data and/or sophisticated pre-training schemes were used, whereas we train our model from scratch on each dataset (except for CUHK01, where CUHK03 was used for pre-training).

\textbf{Bilinear CNNs.} Bilinear convolutional networks (Bilinear CNNs), introduced in \cite{lin2015bilinear} achieved state-of-the-art results for a number of fine-grained recognition tasks, and have also shown potential for face verification \cite{roychowdhury2015face}. Bilinear CNNs consists of two CNNs (where the input of these two CNNs is the same image) without fully-connected layers. The outputs of these two streams are combined in a special way via bilinear pooling. In more detail, the outer product of deep features are calculated for each spatial location, resulting in the quadratic number of feature maps, to which sum pooling over all locations is then performed. The resulting orderless image descriptor is then used in subsequent processing steps. For example, in \cite{lin2015bilinear} and \cite{roychowdhury2015face} it is normalized and fed into the softmax layer for classification. An intuition given in \cite{lin2015bilinear} is that the two CNN streams combined by bilinear operation may correspond to part and texture detectors respectively. This separation may facilitate localization when significant pose variation is present without the need for any part labeling of the training images. Our approach evaluates bilinear CNNs for the person re-identification tasks and improves this architecture by suggesting its multi-region variant.




