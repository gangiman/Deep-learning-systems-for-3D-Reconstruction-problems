\newcommand{\histroot}{Chapters/histloss}

\chapter{Learning Deep Embeddings with Histogram Loss}
\label{chapt:hist}
 


%todo something about the importance of the loss function compared to architecture from israel paper?
\section{Motivation}
%: objective functions for similarity learning

% The problem of learning deep embeddings is considered in this chapter. Usually, such learning is performed using siamese architectures described in  \sect{intro_siamese} to solve the similarity estimation \pr{similarity_estimation}. Under this approach, complex input patterns (\eg{} images) are mapped into a high-dimensional space through a chain of feed-forward transformations, while the parameters of the transformations are learned from a large amount of supervised data. The \textit{objective} of the learning process is to achieve the proximity of semantically related patterns (\eg{} images of the same person) and avoid the proximity of semantically unrelated (\eg{} images of different people) in the target space. It should be noted that this work is focused on simple similarity measures such as Euclidean distance or scalar products, as they allow fast evaluation.

% The formulation of an objective function for learning embeddings depends on the exact formulation of \pr{similarity_estimation}. It can be formalized in several different ways depending on the setting: 
% \begin{enumerate}
% \item \label{assumption_1} whether we need all the positive pairs to have higher similarity values than \textit{all} the negative pairs,

% \item \label{assumption_2} or we only need this condition fulfilled for each separate identity. 
% \end{enumerate}

In more detail, the latter means that positive pairs ${(x_i^a, x_j^a)}_{i,j}$ that consist of images of some identity $a$ should have the higher similarity value compared only to corresponding negative pairs ${(x_i^a, x_k^{n})}_{i,k}$, that contain one image for the identity $a$ and the second image - for some other identity $n$. 

According to one of the two described assumptions, the training data for the similarity-learning tasks may be formed in one of the following basic ways:

\begin{itemize}
    \item a set of positive pairs and a set of negative pairs:
        \begin{equation}
          \begin{aligned}
          \label{c:pairs_constraints}
                   S = \{(x_i^a, x_j^a): x_i^a \text{ and }& x_j^a \text{ are similar}\}, & \\
                   D = \{(x_i^b, x_j^c): x_i^b \text{ and }& x_j^c \text{ are not similar}\};&\\
          \end{aligned}
        \end{equation}

     
    \item a set of triplets:
     \begin{equation}
     \label{c:triplets_constraints}
          R = \{(x_i^a, x_j^a, x_k^n): x_i^a \text{ is more similar to } x_j^p \text{ than to } x_k^n\}; 
     \end{equation}
     
     \item a set of quadruplets:
     \begin{equation}
         \label{c:quadruplets_constraints}
          Q = \{(x_i^a, x_j^a, x_k^b, x_l^c): x_i^a \text{ and } x_j^a \text{ are more similar than } x_k^b \text{ and } x_l^c\}.
      \end{equation}
\end{itemize}

\bigskip\textbf{Training with pairs and quadruplets}\\
The training data forms \ref{c:pairs_constraints} and \ref{c:quadruplets_constraints} correspond to the assumption \ref{assumption_1}. The pairwise data \ref{c:pairs_constraints} require comparing the similarity of each pair to some pre-defined threshold in the process of training (such threshold should separate similarity values of positive and negative pairs). The using of quadruplet data \ref{c:quadruplets_constraints} usually implies comparing the similarity values between the independent pairs (so no predefined threshold is necessary). While pairwise data case is more rigid and requires more assumptions about the data (the threshold value), handling of quadruplet data  is more computationally expensive (number of comparisons will be quadratic in the number of pairs).
%examples

The pairwise form of training data is widely used in metric learning \citep{xing2003distance,globerson2006metric,davis2007information,koestinger2012large,liao2015person,mignon2012pcca}, and also for training siamese architectures by \citep{Bromley93,hadsell2006dimensionality}, including those for face verification by \citep{Sun14, hu2014discriminative} and person re-identification by \citep{yi2014deep, Yi14}.

Quadruplet-based learning is less popular. The metric learning method of \citep{law2013quadruplet} is one of the examples under this category. 
The loss function suggested by \citep{Tadmor2016LearningAM} (it was published simultaneously with the corresponding results of this work) on deep face verification can also be considered quadruplet-based. The reason is that the threshold for separating similarities of positive and negative pairs is a learned parameter in this work. 


\bigskip\textbf{Training with triplets}\\
The triplet form of training data \ref{c:triplets_constraints} corresponds to the assumption \ref{assumption_2}. Differently to pairwise \ref{c:pairs_constraints} and quadruplet data \ref{c:quadruplets_constraints}, for triplets, only relative similarities/distances are compared. This means that the neighborhoods corresponding to different training classes are allowed to be of a different radius in the descriptor space. This is in contrast to using pairs and quadruplets, which implies that all distance (negative similarity) value for positive pairs should not exceed a  certain threshold. 

Training data organized as triplets are usually used for ranking formulations: either linear ranking functions like in methods by
\citep{joachims2002optimizing,prosser2010person,kuo2013person,paisitkriangkrai2015learning} or non-linear ranking functions parametrized by CNN \citep{chen2016deep}. Triplet-based approach is also used for metric learning \citep{schultz2004learning,weinberger2009distance}, and for training siamese neural networks for re-identification \citep{Song16} and even for the verification formulation of the face recognition problem \citep{SchroffKP15,parkhi2015deep}. 
%something about sampling?? but it is also important for pairwise losses, so it probably should not be mentioned
%maybe computational complexity?

\bigskip\textbf{Classification objective}\\
%from the histloss paper
It has been observed in \citep{Krizhevsky12} and confirmed later in multiple works (\eg{} \citep{Razavian14}) that deep networks trained for classification can be used for deep embedding. In particular, it is sufficient to consider an intermediate representation arising in one of the last layers of the deep network. In human recognition task, identity numbers can be used as class labels in order to apply the classification training. \citep{Taigman14} dramatically improved the results for face verification using the classification objective for training a CNN. \citep{parkhi2015deep} use pre-training for classification followed by triplet-based training for the same task. 

Overall, using classification objectives can help the final results \citep{Sun14}, however, this work considers objective functions that are specially designed for learning deep embeddings. 

\bigskip\indent\textbf{Sampling}\\
It has been later demonstrated in literature \citep{sohn2016improved, wu2017sampling}, that sampling scheme for training tuples may be very important and can affect the results. In this chapter, we, however, use simple sampling: we generate all the possible pairs within the mini-batch.



%using pre-trained on imagenet networks? face , re-id, instance retrieval? paper ME (pretrained CNN features are bad for reid?)

%tell what is bad about triplets? computational complexity?
\bigskip\textbf{Motivation for a new loss function}\\
As it was mentioned in the several last paragraphs, the training objectives for embedding learning are most often based on using point-wise constraints \ref{c:pairs_constraints}, \ref{c:triplets_constraints}, \ref{c:quadruplets_constraints}. 
This leads to 
\begin{itemize}
    \item either presence of hyperparameters, (like thresholds, whose optimal values can vary for different training datasets),
    \item or using more complex structures of training data (like triplets or quadruplets).
\end{itemize}  

In this chapter a new loss function that helps to avoid both of these issues is suggested. In designing this function we strive to avoid highly-sensitive parameters such as margins or thresholds of any kind. While processing a batch of data points, the proposed loss is computed in two stages. 
\begin{enumerate}
\item Firstly, the two one-dimensional distributions of similarities in the embedding space are estimated, one corresponding to similarities between matching (\textit{positive}) pairs, the other corresponding to similarities between non-matching (\textit{negative}) pairs. The distributions are estimated in a simple non-parametric way (as histograms with linearly-interpolated values-to-bins assignments).
\item In the second stage, the overlap between the two distributions is computed by estimating the probability that the two points sampled from the two distribution are in a wrong order, i.e.\ that a random negative pair has a higher similarity than a random positive pair.
\end{enumerate}

It should be noted that the way of estimating the overlap between the similarity distributions in the second stage is quite important. The reason we choose the reverse order probability described above is that this loss forces the similarity distribution for positive pairs to the right and the distribution for negative pairs - to the left with respect to the similarity value axis. The latter means that during the process of training, each positive pair gets a pulling signal and each negative pair - a repelling signal. For such measures as Kullback-Leibler divergence \citep{kullback1951information} or Bhattacharyya distance \citep{bhattacharyya1943measure}, this may not be a case, especially if the similarity distributions given by untrained network are highly overlapping. The problem is that, in contrast to the suggested method, these measures do not imply the ordering of distribution means. However, it should be noted that this problem could be fixed by introducing an additional optimization objective, \eg{} the difference between means of the two distributions. 



The two stages are implemented in a piecewise-differentiable manner, thus allowing to minimize the loss (i.e.\ the overlap between distributions) using standard backpropagation.
The number of bins in the histograms is the only tunable parameter associated with our loss, and it can be set according to the batch size independently of the data itself. In the experiments, we fix this parameter (and the batch size) and demonstrate the versatility of the loss by applying it to four different image datasets of varying complexity and nature. Comparing the new loss to state-of-the-art reveals its favorable performance. Overall, we hope that the proposed loss will be used as an ``out-of-the-box'' solution for learning deep embeddings that requires little tuning and leads to close to the state-of-the-art results.


% \begin{abstract}
% We suggest a loss for learning deep embeddings. The new loss does not introduce parameters that need to be tuned and results in very good embeddings across a range of datasets and problems. The loss is computed by estimating two distribution of similarities for positive (matching) and negative (non-matching) sample pairs, and then computing the probability of a positive pair to have a lower similarity score than a negative pair based on the estimated similarity distributions. We show that such operations can be performed in a simple and piecewise-differentiable manner using 1D histograms with soft assignment operations. This makes the proposed loss suitable for learning deep embeddings using stochastic optimization. In the experiments, the new loss performs favourably compared to recently proposed alternatives. 
% \end{abstract}



% %-------------------------------------------------------------------------
% \section{Introduction}

% Deep feed-forward embeddings play a crucial role across a wide range of tasks and applications in image retrieval \citep{Krizhevsky12,Razavian14,Arandjelovic15}, biometric verification \citep{Bromley93,Chopra05,Taigman14,Sun14,Schroff15,parkhi2015deep,Yi14}, visual product search \citep{Song16}, finding sparse and dense image correspondences \citep{Simo15,Zbontar15}, etc. Under this approach, complex input patterns (\eg{} images) are mapped into a high-dimensional space through a chain of feed-forward transformations, while the parameters of the transformations are learned from a large amount of supervised data. The \textit{objective} of the learning process is to achieve the proximity of semantically-related patterns (\eg{} faces of the same person) and avoid the proximity of semantically-unrelated (\eg{} faces of different people) in the target space. In this work, we focus on simple similarity measures such as Euclidean distance or scalar products, as they allow fast evaluation, the use of approximate search methods, and ultimately lead to faster and more scalable systems. 

% Despite the ubiquity of deep feed-forward embeddings, learning them still poses a challenge and is relatively poorly understood. While it is not hard to write down a loss based on tuples of training points expressing the above-mentioned objective, optimizing such a loss rarely works ``out of the box'' for complex data. This is evidenced by the broad variety of losses, which can be based on pairs, triplets or quadruplets of points, as well as by a large number of optimization tricks employed in recent works to reach state-of-the-art, such as pretraining for the classification task while restricting fine-tuning to top layers only~\citep{Taigman14,parkhi2015deep}, combining the embedding loss with the classification loss~\citep{Sun14}, using complex data sampling such as mining ``semi-hard'' training triplets \citep{Schroff15}. Most of the proposed losses and optimization tricks come with a certain number of tunable parameters, and the quality of the final embedding is often sensitive to them.

% Here, we propose a new loss function for learning deep embeddings. In designing this function we strive to avoid highly-sensitive parameters such as margins or thresholds of any kind. While processing a batch of data points, the proposed loss is computed in two stages. Firstly, the two one-dimensional distributions of similarities in the embedding space are estimated, one corresponding to similarities between matching (\textit{positive}) pairs, the other corresponding to similarities between non-matching (\textit{negative}) pairs. The distributions are estimated in a simple non-parametric ways (as histograms with linearly-interpolated values-to-bins assignments). In the second stage, the overlap between the two distributions is computed by estimating the probability that the two points sampled from the two distribution are in a wrong order, i.e.\ that a random negative pair has a higher similarity than a random positive pair. The two stages are implemented in a piecewise-differentiable manner, thus allowing to minimize the loss (i.e.\ the overlap between distributions) using standard backpropagation. The number of bins in the histograms is the only tunable parameter associated with our loss, and it can be set according to the batch size independently of the data itself. In the experiments, we fix this parameter (and the batch size) and demonstrate the versatility of the loss by applying it to four different image datasets of varying complexity and nature. Comparing the new loss to state-of-the-art reveals its favourable performance. Overall, we hope that the proposed loss will be used as an ``out-of-the-box'' solution for learning deep embeddings that requires little tuning and leads to close to the state-of-the-art results.
% %---------------------------------------

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{\histroot/figures/nPLoss.pdf}
    \caption{The histogram loss computation for a batch of examples 
    (color-coded; same color indicates matching samples). After the batch (left) is embedded into a high-dimensional space by a deep network (middle), we compute the histograms of similarities of positive (top-right) and negative pairs (bottom-right). We then evaluate the integral of the product between the negative distribution and the cumulative density function for the positive distribution (shown with a dashed line), which corresponds to a probability that a randomly sampled positive pair has smaller similarity than a randomly sampled negative pair. Such histogram loss can be minimized by backpropagation. The only associated parameter of such loss is the number of histogram bins, to which the results have very low sensitivity.}
    \label{fig:scheme}
\end{figure}


% \section{Related work}

% Recent works on learning embeddings use deep architectures (typically ConvNets \citep{LeCun89,Krizhevsky12}) and stochastic optimization. Below we review the loss functions that have been used in recent works.% Overall, previous works consider mapping from input patterns to a unit sphere in a high-dimensional space (normalized descriptors). Normalization to a unit length has been invariably reported to improve the performance both for deep and for ``shallow'' embeddings \citep{Perronnin10}. The training points are assumed to come with some sort of supervision, typically with class labels, so that for any pair of samples it is known whether they should end up close in the embedding space (positive pair) or whether they should be separated by some distance (negative pair). Some loss functions (including ours) can be extended to a weaker form of supervision, where ``same'' or ``different'' relations are known for only subset of pairs.

% {\bf Classification losses.} It has been observed in \citep{Krizhevsky12} and confirmed later in multiple works (\eg{} \citep{Razavian14}) that deep networks trained for classification can be used for deep embedding. In particular, it is sufficient to consider an intermediate representation arising in one of the last layers of the deep network. The normalization is added post-hoc. Many of the works mentioned below pre-train their embeddings as a part of the classification networks.

% {\bf Pairwise losses.} Methods that use pairwise losses sample pairs of training points and score them independently. The pioneering work on deep embeddings \citep{Bromley93} penalizes the deviation from the unit cosine similarity for positive pairs and the deviation from $-1$ or $-0.9$ for negative pairs.
% Perhaps, the most popular of pairwise losses is the \textit{contrastive} loss \citep{Chopra05,Simo15}, which minimizes the distances in the positive pairs and tries to maximize the distances in the negative pairs as long as these distances are smaller than some margin $M$. Several works pointed to the fact  that attempting to collapse all positive pairs may lead to excessive overfitting and therefore suggested losses that mitigate this effect, \eg{} a double-margin contrastive loss \citep{Lin15}, which drops to zero for positive pairs as long as their distances fall beyond the second (smaller) margin. Finally, several works use non-hinge based pairwise losses such as log-sum-exp and cross-entropy on the similarity values that softly encourage the similarity to be high for positive values and low for negative values (\eg{} \  \citep{Yi14,Taigman14}). The main problem with pairwise losses is that the margin parameters might be hard to tune, especially since the distributions of distances or similarities can be changing dramatically as the learning progresses. While most works ``skip'' the burn-in period by initializing the embedding to a network pre-trained for classification \citep{Taigman14}, \citep{Sun14} further demonstrated the benefit of admixing the classification loss during the fine-tuning stage (which brings in another parameter).

% {\bf Triplet losses.} While pairwise losses care about the absolute values of distances of positive and negative pairs, the quality of embeddings ultimately depends on the relative ordering between positive and negative distances (or similarities). Indeed, the embedding meets the needs of most practical applications as long as the similarities of positive pairs are greater than similarities of negative pairs \citep{Schultz04,Weinberger09}. The most popular class of losses for metric learning therefore consider triplets of points $x_0,x_+,x_-$, where $x_0,x_+$ form a positive pair and $x_0,x_-$ form a negative pair and measure the difference in their distances or similarities. Triplet-based loss can then \eg{} be aggregated over all triplets using a hinge function of these differences. Triplet-based losses are popular for large-scale embedding learning \citep{Chechik10} and in particular for deep embeddings~\citep{parkhi2015deep,Schroff15,Qian15,Zbontar15,Song16}. Setting the margin in the triplet hinge-loss still represents the challenge, as well as sampling ``correct'' triplets, since the majority of them quickly become associated with zero loss. On the other hand, focusing sampling on the hardest triplets can prevent efficient learning \citep{Schroff15}. Triplet-based losses generally make learning less constrained than pairwise losses. This is because for a low-loss embedding, the characteristic distance separating positive and negative pairs can vary across the embedding space (depending on the location of $x_0$), which is not possible for pairwise losses. In some situations, such added flexibility can increase overfitting.

% {\bf Quadruplet losses.} Quadruplet-based losses are similar to triplet-based losses as they are computed by looking at the differences in distances/similarities of positive pairs and negative pairs. In the case of quadruplet-based losses, the compared positive and negative pairs do not share a common point (as they do for triplet-based losses). Quadruplet-based losses do not allow the flexibility of triplet-based losses discussed above (as they includes comparisons of positive and negative pairs located in different parts of the embedding space). At the same time, they are not as rigid as pairwise losses, as they only penalize the relative ordering for negative pairs and positive pairs. Nevertheless, despite these appealing properties, quadruplet-based losses remain rarely-used and confined to ``shallow'' embeddings \citep{Law13,Zheng13}. We are unaware of deep embedding approaches using quadruplet losses. A potential problem with quadruplet-based losses in the large-scale setting is that the number of all quadruplets is even larger than the number of triplets. Among all groups of losses, our approach is most related to quadruplet-based ones, and can be seen as a way to organize learning of deep embeddings with a quarduplet-based loss in an efficient and (almost) parameter-free manner.


%---------------------------------------
\section{Histogram loss}
\label{sect:loss}

We now describe our loss function and then relate it to the quadruplet-based loss. Our loss (\fig{scheme}) is defined for a batch of examples $X = \{x_1,x_2,\dots x_N\}$ and a deep feedforward network $f(\cdot;\theta)$, where $\theta$ represents learnable parameters of the network. We assume that the last layer of the network performs length-normalization, so that the embedded vectors $\{y_i = f(x_i;\theta)\}$ are $L2$-normalized.

We further assume that we know which elements should match to each other and which ones are not. Let $m_{ij}$ be $+1$ if $x_i$ and $x_j$ form a positive pair (correspond to a match) and $m_{ij}$ be $-1$ if $x_i$ and $x_j$ are known to form a negative pair  (these labels can be derived from class labels or be specified otherwise). Given $\{m_{ij}\}$ and $\{y_i\}$ we can estimate the two probability distributions $\prp$ and $\prm$ corresponding to the similarities in positive and negative pairs respectively. In particular $\Sp = \{ s_{ij} = \spr{x_i}{x_j}\,|\,m_{ij} = +1\}$ and $\Sm = \{s_{ij} = \spr{x_i}{x_j}\,|\,m_{ij} = -1\}$ can be regarded as sample sets from these two distributions. Although samples in these sets are not independent, we keep all of them to ensure a large sample size. 

Given sample sets $\Sp$ and $\Sm$, we can use any statistical approach to estimate $\prp$ and $\prm$. The fact that these distributions are one-dimensional and bounded to $[-1;+1]$ simplifies the task. Perhaps, the most obvious choice in this case is fitting simple histograms with uniformly spaced bins, and we use this approach in our experiments. We therefore consider $R$-dimensional histograms $\Hp$ and $\Hm$, with the nodes $t_1=-1,t_2,\dots,t_R=+1$ uniformly filling $[-1;+1]$ with the step $\Delta=\frac{2}{R-1}$. We estimate the value $\hp_r$ of the histogram $\Hp$ at each node as:
\begin{equation} \label{eq:hist}
    \hp_r = \frac{1}{|\Sp|}\sum_{(i,j)\,:\,m_{ij}=+1} \delta_{i,j,r}\, 
\end{equation}
where $(i,j)$ spans all positive pairs of points in the batch. The weights $\delta_{i,j,r}$ are chosen so that each pair sample is assigned to the two adjacent nodes:
\begin{equation} \label{eq:histweight}
\delta_{i,j,r} = \begin{cases} 
    (s_{ij}-t_{r-1})/\Delta,\; &\text{if $s_{ij} \in [t_{r-1};t_r]$},\\
    (t_{r+1}-s_{ij})/\Delta,\; &\text{if $s_{ij} \in [t_r;t_{r+1}]$},\\
    0,\; &\text{otherwise}\,.
    \end{cases}
\end{equation}
 We thus use linear interpolation for each entry in the pair set, when assigning it to the two nodes. The estimation of $\Hm$ proceeds analogously. Note, that the described approach is equivalent to using ''triangular'' kernel for density estimation; other kernel functions can be used as well. %\citep{GVK233599487}.

Once we have the estimates for the  distributions $\prp$ and $\prm$, we use them to estimate the probability of the similarity in a random negative pair to be more than the similarity in a random positive pair ( \textit{the probability of reverse}). Generally, this probability can be estimated as:
\begin{equation} \label{eq:reverse}
p_\text{reverse} = \int_{-1}^{1} \prm(x) \left[\int_{-1}^x \prp(y)\,dy\right]\, dx \, =
\int_{-1}^{1} \prm(x)\,\Phi^{+}(x)\, dx = \mathbb{E}_{x \sim \prm}[\Phi^{+}(x)]\,,
\end{equation}
where $\Phi^{+}(x)$ is the CDF (cumulative density function) of $\prp(x)$. The integral \eq{reverse} can then be approximated and computed as:
\begin{equation} \label{eq:loss}
L(X,\theta) = \sum_{r=1}^R \left( \hm_r \sum_{q=1}^r \hp_q  \right)= \sum_{r=1}^R  \hm_r \phi^{+}_r\,,
\end{equation}
where $L$ is our loss function (the \textit{histogram loss}) computed for the batch $X$ and the embedding parameters $\theta$, which approximates the reverse probability; $\phi^{+}_r=\sum_{q=1}^r \hp_q$ is the cumulative sum of the histogram $\Hp$.

Importantly, the loss \eq{loss} is differentiable w.r.t.\ the pairwise similarities $s\in\Sp$ and $s\in\Sm$. Indeed, it is straightforward to obtain $\frac{\partial L}{\partial \hm_r} = \sum_{q=1}^{r} \hp_q$ and $\frac{\partial L}{\partial \hp_r}=\sum_{q=r}^{R} \hm_q$ from \eq{loss}. Furthermore, from \eq{hist} and \eq{histweight} it follows that:
\begin{equation} \label{eq:histder}
\frac{\partial \hp_r}{\partial s_{ij}} = \begin{cases} 
     \frac{+1}{\Delta |\Sp|} ,\; &\text{if $s_{ij} \in [t_{r-1};t_r]$},\\
     \frac{-1}{\Delta |\Sp|},\; &\text{if $s_{ij} \in [t_r;t_{r+1}]$},\\
    0,\; &\text{otherwise}\,,
    \end{cases}
\end{equation}
for any $s_{ij}$ such that $m_{ij}=+1$ (and analogously for $\frac{\partial \hm_r}{\partial s_{ij}}$). Finally, $\frac{\partial s_{ij}}{\partial x_i}=x_j$ and $\frac{\partial s_{ij}}{\partial x_j}=x_i$. One can thus backpropagate the loss to the scalar product similarities, then further to the individual embedded points, and then further into the deep embedding network.

{\bf Relation to quadruplet loss.} \\
Our loss first estimates the probability distributions of similarities for positive and negative pairs in a semi-parametric ways (using histograms), and then computes the probability of reverse using these distributions via equation \eq{loss}. An alternative and purely non-parametric way would be to consider all possible pairs of positive and negative pairs contained in the batch and to estimate this probability from such set of pairs of pairs. This would correspond to evaluating a quadruplet-based loss similarly to \citep{Law13,Zheng13}. The number of pairs of pairs in a batch, however tends to be quartic (fourth degree polynomial) of the batch size, rendering exhaustive sampling impractical. This is in contrast to our loss, for which the separation into two stages brings down the complexity to quadratic in batch size. Another efficient loss based on quadruplets is introduced in \citep{Tadmor2016LearningAM}. The training is done pairwise, but the threshold separating positive and negative pairs is also learned.

We note that quadruplet-based losses as in \citep{Law13,Zheng13} often encourage the positive pairs to be more similar than negative pairs by some non-zero margin. It is also easy to incorporate such non-zero margin into our method by defining the loss to be:
\begin{equation} \label{eq:margin}
L_\mu(X,\theta) = \sum_{r=1}^R \left( \hm_r \sum_{q=1}^{r+\mu} \hp_q  \right)\,,
\end{equation}
where the new loss effectively enforces the margin $\mu\,\Delta$. We however do not use such modification in our experiments (preliminary experiments do not show any benefit of introducing the margin).


{\bf Computational complexity.} \\
The complexity of computing the Histogram loss in the batch size $n$ and the number of histogram bins $R$ is $O(n^2 \log{R} + R)$, as first each pair is assigned to the corresponding bin, then the estimates of two cumulative density functions are computed, and finally, the loss \eq{loss} is computed.




\begin{figure}
\begin{center}
\begin{tabular}{c}
  
      \includegraphics[width=0.7\textwidth]{\histroot/figures/birds_recall_grids.pdf}\\
      (a)\\
    % \begin{subfigure}{0.33\textwidth}
    %   \includegraphics[width=\linewidth]{\histroot/figures/cuhk03_labeled_split1_recall_grids.pdf}
    % \end{subfigure}

        \includegraphics[width=0.7\textwidth]{\histroot/figures/cuhk03_labeled_split1_recall_batch_size.pdf}\\
        (b)\\
\end{tabular}
    \caption{(a) - Recall@K for the CUB-200-2011 dataset for the Histogram loss \eq{loss}. Different curves correspond to variable histogram step $\Delta$, which is the only parameter inherent to our loss.  The curves are very similar for CUB-200-2011. (b) - Recall@K for the CUHK03 labeled dataset for different batch sizes. Results for batch size $256$ is uniformly better than those for smaller values.
    }
    \label{fig:additional}
\end{center}
\end{figure}





\section{Experiments}
In this section we present the results of embedding learning. %for four image datasets: two datasets for person re-identification, one for product image search, and one for fine-grained bird recognition. 
We compare our loss to state-of-the-art pairwise and triplet losses, which have been reported in recent works to give state-of-the-art performance on these datasets.

{\bf Baselines.} \\
In particular, we have evaluated the Binomial Deviance loss \eq{bindev} \citep{Yi14}. While we are aware only of its use in person re-identification approaches, in our experiments it performed very well for product image search and bird recognition significantly outperforming the baseline pairwise (contrastive) loss reported in \citep{Song16}, once its parameters are tuned. %The binomial deviance loss is defined as:

% \begin{equation}
% \label{eq:bindev}
% J_{dev} = \sum_{i,j \in I} w_{i,j} \ln (\exp^{-\alpha (s_{i,j}-\beta) m_{i,j}} +1 ), 
% \end{equation}
% where $I$ is the set of training image indices, and $s_{i,j}$ is the similarity measure between $i$th and $j$th images (i.e.\ $s_{i,j} = cosine(x_{i}, x_{j}$).

% Furthermore,  $m_{i,j}$ and $w_{i, j}$ are the learning supervision and scaling factors respectively:
% \begin{equation}
%     \label{eq:bindev_weights}
%     m_{i, j} = \left\{
% 	\begin{array}{l l}
% 		1,  ${if $(i,j)$ is a positive pair,}$ &\\
% 	   -C,  ${if $(i,j)$ is a negative pair,}$ &
% 	\end{array}\right.
%      w_{i, j}= \left\{
% 	\begin{array}{l l}
% 		\frac {1}{n_1},  ${if $(i,j)$ is a positive pair,}$ &\\
% 		\frac {1}{n_2},  ${if $(i,j)$ is a negative pair,}$ &
% 	\end{array}\right.
% \end{equation}	

% where $n_1$ and $n_2$ are the number of positive and negative pairs in the training set (or mini-batch) correspondingly, $\alpha$ and $\beta$ are hyper-parameters. Parameter $C$ is the negative cost for balancing weights for positive and negative pairs that was introduced in \citep{Yi14}. 
Our experimental results suggest that the quality of the embedding is sensitive to parameter $C$ of the Binomial Deviance loss. Therefore, in the experiments we report results for the two versions of the loss: with $C=10$ that is close to optimal for re-identification datasets, and with $C=25$ that is close to optimal for the product and bird datasets.

We have also computed the results for the Lifted Structured Similarity Softmax (LSSS) loss \eq{lifted} \citep{Song16} on CUB-200-2011 \citep{Wah11} and Online Products \citep{Song16} datasets  and additionally applied it to  re-identification datasets. Lifted Structured Similarity Softmax loss is triplet-based and uses sophisticated triplet sampling strategy that was shown in \citep{Song16} to outperform standard triplet-based loss. 


Additionally, we performed experiments for the triplet loss \citep{SchroffKP15} that uses ``semi-hard negative'' triplet sampling. Such sampling considers only triplets violating the margin, but still having the positive distance smaller than the negative distance.


\begin{figure}
\begin{center}
\begin{tabular}{c}

        \includegraphics[width=0.7\textwidth]{\histroot/figures/birds_recall.pdf}\\
        (a)\\
    %    \caption{}
    %    \label{fig:birds_recall}


        \includegraphics[width=0.7\textwidth]{\histroot/figures/products_recall.pdf}\\
        (b)\\
    %    \caption{}
    %    \label{fig:products_recall}
\end{tabular}
    \caption{Recall@K for (a) - CUB-200-2011 and (b) - Online Products datasets for different methods. Results for the Histogram loss \eq{loss}, Binomial Deviance \eq{bindev}, LSSS \eq{lifted} \citep{Song16} and Triplet \eq{triplet} \citep{SchroffKP15} losses are present. Binomial Deviance loss for $C=25$ outperforms all other methods. The best-performing method is Histogram loss. We also include results for contrastive and triplet losses from \citep{Song16}.
    }
    \label{fig:birds_products}
\end{center}
\end{figure}


%------------- to main into------------
\noindent\textbf{Datasets and evaluation metrics.}\\
We have evaluated the above mentioned loss functions on the four datasets : CUB200-2011 \citep{Wah11}, CUHK03 \citep{Li14}, Market-1501 \citep{Zheng15} and Online Products \citep{Song16}. All these datasets have been used for evaluating methods of solving embedding learning tasks. 
Specifically, the Online Products dataset introduced in \citep{Song16} along with CUB200-2011 was used for training a deep image retrieval model \citep{Song16}. CUHK03 and  Market-1501 are the largest datasets for person re-identification available at the moment. 

Commonly, for the retrieval tasks training and testing are done using disjoint sets of classes. For many datasets, including above mentioned ones, the standard splits for evaluation are provided.
The CUB-200-2011 dataset includes 11,788 images of 200 classes corresponding to different birds species. As in \citep{Song16} we use the first 100 classes for training
(5,864 images) and the remaining classes for testing
(5,924 images).
% %Online products
The Online Products dataset includes 120,053 images of 22,634 classes. Classes correspond to a number of online products from eBay.com. There are approximately 5.3 images for each product. We used the standard split from \citep{Song16}: 11,318 classes (59,551 images)  are used for training and 11,316 classes (60,502 images) are used for testing.
The images from the CUB-200-2011 and the Online Products datasets are resized to 256 by 256, keeping the original aspect ratio (padding is done when needed). The Recall@K metric is calculated for the set of queries consisting of all the images in the test set.

% %CUHK03
% The CUHK03 dataset is commonly used for the person re-identification task. It includes 13,164 images of 1,360 pedestrians captured from 3 pairs of cameras. Each identity is observed by two cameras and has 4.8 images in each camera on average.  Following most of the previous works we use the ``CUHK03-labeled'' version of the dataset with manually-annotated bounding boxes. According to the CUHK03 evaluation protocol, 1,360 identities are split into 1,160 identities for training, 100 for validation and 100 for testing. We use the first split from the CUHK03 standard split set which is provided with the dataset. 
% %MVS
% % The Multi-view Stereo Correspondence dataset (MVS) contains over 1.5M grayscale image patches of size 64$\times$64. The patches are sampled around more than 500K 3D points in different views. There are three patches subsets in MVS: patches are sampled from 3D reconstructions of the Statue of Liberty (New York), Notre Dame (Paris) and Half Dome (Yosemite). For each of the three subsets a number of patches pairs with their ground truth labels are provided for evaluation. The common protocol used for MVS is training on the one subset and testing on the two other subsets.
% %The second re-identification dataset we use is Market-1501, introduced in \citep{Zheng15}.
% %(See figure \ref{fig:datasets_market} for sample images). 
% The Market-1501 dataset includes 32,643 images of 1,501 pedestrians, each pedestrian is captured by several cameras (from two to six). The dataset is divided randomly  into the test set of 750 identities and the train set of 751 identities. %We use 51 identities from the train set for validation. In the test set there are several query images for each identity: one image from each camera is selected for each pedestrian. When testing, all the images of the query person captured from the same camera as the query image are excluded from the gallery.% Manually drawn bounding boxes are used for query images, and automatically detected ones for training images as well as for the gallery images in the test set. %Market-1501 also contains ''distractor'' images as a part of the gallery set. These images correspond to false detections and are ''negatives'' to all the queries, which makes evaluation on the dataset even more challenging. 
% %the test data

%\footnote{Recall@K is the probability of getting the right match among first K gallery candidates sorted by similarity.}
Following \citep{Song16,Yi14,Zheng15}, we report Recall@K metric for all the datasets.  For CUB-200-2011 and Online products,  every test image is used as the query in turn and remaining images are used as the gallery correspondingly. In contrast, for CUHK03 \textit{single-shot} results are reported. This means that one image for each identity from the test set is chosen randomly in each of its two camera views. Recall@K values for 100 random query-gallery sets are averaged to compute the final result for a given split. For the Market-1501 dataset, we use the \textit{multi-shot} protocol (as is done in most other works), as there are many images of the same person in the gallery set.
%----------------------------------
%\begin{wrapfigure}
% \begin{figure}
% \centering
%   \includegraphics[width=0.4\linewidth]{\histroot/figures/birds_recall_grids.pdf}
%          \caption{Recall@K for the CUB-200-2011 dataset for the Histogram loss \eq{loss}. Different curves correspond to variable histogram step $\Delta$, which is the only parameter inherent to our loss.  The curves are very similar despite an order of magnitude variation.}
%          \label{fig:birds_recall_grids}
%   % \end{left}
 
% \end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{c}
 
        \includegraphics[width=0.7\textwidth]{\histroot/figures/cuhk03_labeled_split1_recall.pdf} \\
        (a) \\
      %  \caption{}
      %  \label{fig:cuhk03_labeled_split1_recall}
  
    
        \includegraphics[width=0.7\textwidth]{\histroot/figures/market_recall.pdf}\\
        (b) \\
      %  \caption{}
      %  \label{fig:market_recall}

\end{tabular}
    \caption{Recall@K for (a) - CUHK03 and (b) -  Market-1501 datasets. The Histogram loss \eq{loss} outperforms Binomial Deviance, LSSS and Triplet losses. }
    \label{fig:reid_recall}   
\end{center}
\end{figure}

\noindent\textbf{Architectures used.}\\
For training on the CUB-200-2011 and the Online Products datasets we used the same architecture as in \citep{Song16}, which conincides with the GoogleNet architecture \citep{szegedy2015going} up to the `pool5' and the inner product layers, while the last layer is used to compute the embedding vectors. The GoogleNet part is pretrained on ImageNet ILSVRC \citep{ILSVRC15} and the last layer is trained from scratch. As in \citep{Song16}, all GoogLeNet layers are fine-tuned with the learning rate that is ten times less than the learning rate of the last layer. We set the embedding size to 512 for all the experiments with this architecture. We reproduced the results for the LSSS loss \eq{lifted} \citep{Song16} for these two datasets. For the architectures that use the Binomial Deviance loss, Histogram loss and Triplet loss the iteration number and the parameters value (for the former) are chosen using the validation set.    


For training on CUHK03 and Market-1501 we used the Deep Metric Learning (DML) architecture introduced in \citep{Yi14}. It has been described in \sect{intro_architectures}. 

\begin{table}%{0.6\textwidth}
\begin{center}
\caption{Final results for CUHK03-labeled and Market-1501. For CUHK03-labeled results for 5 random splits were averaged. Batch of size 256 was used for both experiments.}\label{tab:reid}
\begin{tabular}{cccccc}\\\toprule  
Dataset      & r = 1     &  r = 5     & r = 10    & r = 15     & r = 20 \\\midrule
 CUHK03      & 65.77 & 92.85 & 97.62 &  98.94 & 99.43\\  %\midrule
 Market-1501 & 59.47 & 80.73 & 86.94 &  89.28 & 91.09 \\  \bottomrule
\end{tabular}
\end{center}
\end{table} 

\noindent\textbf{Implementation details.}\\
For all the experiments with loss functions \eq{loss} and \eq{bindev} we used quadratic number of pairs in each batch (all the pairs that can be sampled from batch). For triplet loss ``semi-hard'' triplets chosen from all the possible triplets in the batch are used. For comparison with other methods the batch size was set to $128$. We sample batches randomly in such a way that there are several images for each  sampled class in the batch. We iterate over all the classes and all the images corresponding to the classes, sampling images in turn. The sequences of the classes and of the corresponding images are shuffled for every new epoch. CUB-200-2011 and Market-1501 include more than ten images per class on average, so we limit the number of images of the same class in the batch to ten for the experiments on these datasets. We used ADAM \citep{Kingma14} for stochastic optimization in all of the experiments. For all losses the learning rate is set to $1e-4$ for all the experiments except ones on the CUB-200-2011 datasets, for which we have found the learning rate of $1e-5$ more effective. 
For the re-identification datasets the learning rate was decreased by 10 after the 100K iterations, for the other experiments learning rate was fixed. The iterations number for each method was chosen using the validation set.

\noindent\textbf{Results.}\\
The Recall@K values for the experiments on CUB-200-2011, Online Products, CUHK03 and Market-1501 are shown in \fig{birds_products} and \fig{reid_recall}. The Binomial Deviance loss \eq{bindev} gives the best results for CUB-200-2011 and Online Products with the $C$ parameter set to $25$. We previously checked several values of $C$ on the CUB-200-2011 dataset and found the value $C=25$ to be the optimal one. We also observed that with smaller values of $C$ the results are significantly worse than those presented in the \fig{birds_products}a (for $C$ equal to $2$ the best Recall@1 is 43.50\%). For CUHK03 the situation is reverse: the Histogram loss gives the boost of 2.64\% over the Binomial Deviance loss with $C=10$ (which we found to be optimal for this dataset). The results are shown in the figure \fig{reid_recall}a. Embedding distributions of the positive and negative pairs from CUHK03 test set for different methods are shown in Figure  \ref{fig:distr}.
For the Market-1501 dataset our method also outperforms the Binomial Deviance loss for both values of $C$. In contrast to the experiments with CUHK03, the Binomial Deviance loss appeared to perform better with $C$ set to $25$ than to $10$ for Market-1501. We have also investigated how the size of the histogram bin affects the model performance for the Histogram loss. As shown in the \fig{additional}a, the results for CUB-200-2011 remain stable for the sizes equal to 0.005, 0.01, 0.02 and 0.04 (these values correspond to 400, 200, 100 and 50 bins in the histograms). In our method,  distributions of similarities of training data are estimated by distributions of similarities within mini-batches. Therefore we also show results for the Histogram loss for various batch size values (\fig{additional}b). The larger batches are more preferable: for CUHK03, Recall@K for batch size equal to $256$ is uniformly better than Recall@K for $128$ and $64$. We also observed similar behaviour for Market-1501. Additionally, we present our final results (batch size set to 256) for CUHK03 and Market-1501 in \tab{reid}. For CUHK03, Rekall@K values for 5 random splits were averaged. To the best of our knowledge, these results corresponded to state-of-the-art on CUHK03 and Market-1501 at the moment of submission. 
To summarize the results of the comparison: the new (Histogram) loss gives the best results on the two person re-identification problems. For CUB-200-2011 and Online Products it came very close to the best loss (Binomial Deviance with $C=25$). Interestingly, the histogram loss uniformly outperformed the triplet-based LSSS loss \eq{lifted} \citep{Song16} in our experiments including two datasets from \citep{Song16}. Importantly, the new loss does not require to tune parameters associated with it (though we have found learning with our loss to be sensitive to the learning rate).








% \begin{wraptable}{r}{5.5cm}
% \caption{Final results for CUHK03-labeled and Market-1501. For CUHK03-labeled results for 5 random splits were used. Batch of size 256 was user for both experiments.}\label{reid}
% \begin{tabular}{\linewidth}{c|ccccc}
%     \hline
%  %  & \multicolumn{2}{c}{CUHK03} & \multicolumn{2}{c}{Market}\\
   
%     % \hline
%     % rank & 128 & 256 & 128 & 256  \\
%     % 1    & 64.15 & 65.77 & 56.62 & 59.47 \\
%     % 5    & 91.66 & 92.85 & 78.92 & 80.73 \\
%     % 10   & 96.97 & 97.62 & 85.15 & 86.94 \\
%     % 15   & 98.58 & 98.94 & 88.27 & 89.28 \\
%     % 20   & 99.26 & 99.43 & 89.79 & 91.09 \\
%     % \hline

%     Dataset     & \multicolumn{5}{c}{rank} \\
%                 & 1     & 5     & 10    & 15     & 20 \\ 
%     CUHK03      & 65.77 & 92.85 & 97.62 &  98.94 & 99.43 \\
%     Market-1501 & 59.47 & 80.73 & 86.94 &  89.28 & 91.09 \\
% \end{tabular}    
% \end{wraptable} 


\begin{figure}

\begin{center}
\begin{tabular}{c}
  
        \includegraphics[width=0.7\textwidth]{\histroot/figures/initial_distr_cuhk03.png} \\
        (a) \\
      %  \caption{}
       % \label{fig:initial}
   
  
        \includegraphics[width=0.7\textwidth]{\histroot/figures/np.png} \\
        (b) \\
     %   \caption{}
     %   \label{fig:np}
  
 
        \includegraphics[width=0.7\textwidth]{\histroot/figures/bindev10.png} \\
        (c) \\
    %    \caption{}
  %      \label{fig:bindev10}
    
  %  \begin{subfigure}{\textwidth}
        \includegraphics[width=0.7\textwidth]{\histroot/figures/lifted.png} \\
        (d) \\
  %      \caption{}
   %     \label{fig:lifted}
   %     \end{subfigure}   
 \end{tabular}
\end{center}      
    
    \caption{Histograms for positive and negative distance distributions on the CUHK03 test set for: (a) Initial state: randomly initialized net, (b) Network training with the Histogram loss, (c) same for the Binomial Deviance loss, (d) same for the LSSS loss. Red is for negative pairs, green is for positive pairs. Negative cosine distance measure is used for Histogram and Binomial Deviance losses, Euclidean distance is used for the LSSS loss. Initially the two distributions are highly overlapped. For the Histogram loss the distribution overlap is less than for the LSSS. }
    \label{fig:distr}    
    

\end{figure}



%-------------------------------------------------------------------------
\section{Conclusion}
In this chapter we have suggested a new loss function for learning deep embeddings, called the Histogram loss. Like most previous losses, it is based on the idea of making the distributions of the similarities of the positive and negative pairs less overlapping. Unlike other losses used for deep embeddings, the new loss comes with virtually no parameters that need to be tuned. It also incorporates information across a large number of quadruplets formed from training samples in the mini-batch and implicitly takes into account all of such quadruplets. We have demonstrated the competitive results of the new loss on a number of datasets. In particular, the Histogram loss outperformed other losses for the person re-identification problem on CUHK03 and Market-1501 datasets. The code for Caffe~\citep{jia2014caffe} is available at: \url{https://github.com/madkn/HistogramLoss}.



\textbf{Acknowledgement:} This research is supported by the Russian Ministry of Science and Education grant RFMEFI57914X0071.

%-------------------------------------------------------------------------
