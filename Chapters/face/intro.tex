\section{Introduction}
\label{sect:intro}

Large training datasets for face recognition are typically harvested from the Internet, where lots of face images with known identities of people in them can be mined in an automatic or semi-automatic way. When trained on such datasets, deep face recognition networks are rivaling and exceeding human face recognition performance, when applied to similar types of images. 

Yet, many, perhaps majority, of practical scenarios require building vision systems that recognize faces in images that are quite different from those harvested from the Internet. These scenarios include surveillance for security purposes, smart home applications, collaborative robotics, which all come with specific camera parameters, camera positioning, lighting conditions that are all quite different and are often of much lower quality than face images typically encountered on the Internet. In this work, we investigate and compare approaches to building face recognition systems that can operate under this strong domain shift, being able to leverage large annotated datasets of Internet face images and videos for the efficient recognition of faces in surveillance data. 

In particular, we study the unsupervised domain adaptation scenario, where face recognition is trained using an annotated Internet face dataset and an unannotated dataset of faces collected from a surveillance camera network with low image quality. We mostly focus on the recent class of methods that consider domain-adaptation at the image level. We thus investigate how image transformation achieved with recent unsupervised image transformation techniques such as CycleGAN~\cite{ZhuPIE17} can be used for face recognition under strong domain shifts. We compare and evaluate several strategies, such as transferring test data to the Internet image domain,  transferring training data to the target domain followed by retraining the network. As a baseline, we also compare to adversarial domain adaptation at feature level \cite{GaninUAGLLML16}. 

Our comparison suggests that image transformation (without explicit modeling of separate degradation factors) can be used successfully for unsupervised domain adaptation of face recognition. We, however, demonstrate that a special care needs to be taken in order to make such domain adaptation work better than baselines, and come up with practical suggestions on how such improvement can be achieved. We will make our test data and models available by the time of publication.


The remainder of the paper is organized as follows. We first give an overview of existing approaches to face recognition techniques in the presence of degradation factors and recent domain adaptation approaches in \sect{related}. Image-level domain transfer and face recognition methods are described in \sect{method}. 
We describe particular datasets and face recognition protocols used for training and evaluation in \sect{datasets}. We define the variants of the training data augmentation compared in this work in \sect{ft}. Then we give the implementation details in \sect{training}. The quantitative comparisons of the described methods and baselines are presented in \sect{results}. Finally, we conclude the work with discussion and summary in \sect{conclusion}.