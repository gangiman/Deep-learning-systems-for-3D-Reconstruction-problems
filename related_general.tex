

Hist:

Recent works on learning embeddings use deep architectures (typically ConvNets \citep{LeCun89,Krizhevsky12}) and stochastic optimization. Below we review the loss functions that have been used in recent works.% Overall, previous works consider mapping from input patterns to a unit sphere in a high-dimensional space (normalized descriptors). Normalization to a unit length has been invariably reported to improve the performance both for deep and for ``shallow'' embeddings \citep{Perronnin10}. The training points are assumed to come with some sort of supervision, typically with class labels, so that for any pair of samples it is known whether they should end up close in the embedding space (positive pair) or whether they should be separated by some distance (negative pair). Some loss functions (including ours) can be extended to a weaker form of supervision, where ``same'' or ``different'' relations are known for only subset of pairs.

{\bf Classification losses.} It has been observed in \citep{Krizhevsky12} and confirmed later in multiple works (e.g.\ \citep{Razavian14}) that deep networks trained for classification can be used for deep embedding. In particular, it is sufficient to consider an intermediate representation arising in one of the last layers of the deep network. The normalization is added post-hoc. Many of the works mentioned below pre-train their embeddings as a part of the classification networks.

{\bf Pairwise losses.} Methods that use pairwise losses sample pairs of training points and score them independently. The pioneering work on deep embeddings \citep{Bromley93} penalizes the deviation from the unit cosine similarity for positive pairs and the deviation from $-1$ or $-0.9$ for negative pairs.
Perhaps, the most popular of pairwise losses is the \textit{contrastive} loss \citep{Chopra05,Simo15}, which minimizes the distances in the positive pairs and tries to maximize the distances in the negative pairs as long as these distances are smaller than some margin $M$. Several works pointed to the fact  that attempting to collapse all positive pairs may lead to excessive overfitting and therefore suggested losses that mitigate this effect, e.g.\ a double-margin contrastive loss \citep{Lin15}, which drops to zero for positive pairs as long as their distances fall beyond the second (smaller) margin. Finally, several works use non-hinge based pairwise losses such as log-sum-exp and cross-entropy on the similarity values that softly encourage the similarity to be high for positive values and low for negative values (e.g.\ \  \citep{Yi14,Taigman14}). The main problem with pairwise losses is that the margin parameters might be hard to tune, especially since the distributions of distances or similarities can be changing dramatically as the learning progresses. While most works ``skip'' the burn-in period by initializing the embedding to a network pre-trained for classification \citep{Taigman14}, \citep{Sun14} further demonstrated the benefit of admixing the classification loss during the fine-tuning stage (which brings in another parameter).

{\bf Triplet losses.} While pairwise losses care about the absolute values of distances of positive and negative pairs, the quality of embeddings ultimately depends on the relative ordering between positive and negative distances (or similarities). Indeed, the embedding meets the needs of most practical applications as long as the similarities of positive pairs are greater than similarities of negative pairs \citep{Schultz04,Weinberger09}. The most popular class of losses for metric learning therefore consider triplets of points $x_0,x_+,x_-$, where $x_0,x_+$ form a positive pair and $x_0,x_-$ form a negative pair and measure the difference in their distances or similarities. Triplet-based loss can then e.g.\ be aggregated over all triplets using a hinge function of these differences. Triplet-based losses are popular for large-scale embedding learning \citep{Chechik10} and in particular for deep embeddings~\citep{Parkhi15,Schroff15,Qian15,Zbontar15,Song16}. Setting the margin in the triplet hinge-loss still represents the challenge, as well as sampling ``correct'' triplets, since the majority of them quickly become associated with zero loss. On the other hand, focusing sampling on the hardest triplets can prevent efficient learning \citep{Schroff15}. Triplet-based losses generally make learning less constrained than pairwise losses. This is because for a low-loss embedding, the characteristic distance separating positive and negative pairs can vary across the embedding space (depending on the location of $x_0$), which is not possible for pairwise losses. In some situations, such added flexibility can increase overfitting.

{\bf Quadruplet losses.} Quadruplet-based losses are similar to triplet-based losses as they are computed by looking at the differences in distances/similarities of positive pairs and negative pairs. In the case of quadruplet-based losses, the compared positive and negative pairs do not share a common point (as they do for triplet-based losses). Quadruplet-based losses do not allow the flexibility of triplet-based losses discussed above (as they includes comparisons of positive and negative pairs located in different parts of the embedding space). At the same time, they are not as rigid as pairwise losses, as they only penalize the relative ordering for negative pairs and positive pairs. Nevertheless, despite these appealing properties, quadruplet-based losses remain rarely-used and confined to ``shallow'' embeddings \citep{Law13,Zheng13}. We are unaware of deep embedding approaches using quadruplet losses. A potential problem with quadruplet-based losses in the large-scale setting is that the number of all quadruplets is even larger than the number of triplets. Among all groups of losses, our approach is most related to quadruplet-based ones, and can be seen as a way to organize learning of deep embeddings with a quarduplet-based loss in an efficient and (almost) parameter-free manner.

Bilinear:
\textbf{Deep CNNs for Re-Identifications.} Several CNN-based methods for person re-identification have been proposed recently\cite{li2014deepreid, yi2014deep, ahmed2015improved, chen2015deep, VariorHW16, VariorSLXW16,SuZX0T16, LiuFQJY17, XiaoLOW16}. Yi~\etal~\cite{yi2014deep} were among the first to evaluate ``siamese'' architectures that accomplishes embedding of pedestrian images into the descriptor space, where they can be further compared using cosine distance. In \cite{yi2014deep}, a peculiar architecture specific to pedestrian images is proposed that includes three independent sub-networks corresponding to three regions (legs, torso, head-and-shoulders). This is done in order to take into account the variability of the statistics of textures, shapes, and articulations between the three regions. Our architecture includes the network of  Yi~\etal~\cite{yi2014deep} as its part.

Apart from \cite{yi2014deep}, \cite{li2014deepreid} and \cite{ahmed2015improved} learn classification networks that can categorize a pair of images as either depicting  the same subjects or different subjects. %In both approaches, the two images are fed into the network, and the difference in their mid-level representation is processed by additional special layers (Patch matching in \cite{li2014deepreid} and cross-input neighbourhood difference in \cite{ahmed2015improved}).% After that, extra convolutional and inner product layers are applied to the combined representation of the pair of images. Finally, the classification for the pair of images is performed using softmax layer. 
The proposed deep learning approaches \cite{ahmed2015improved, yi2014deep, li2014deepreid}, while competitive, do not clearly outperform more traditional approaches based on ``hand-engineered'' features \cite{paisitkriangkrai2015learning, zhao2014person}.

Unfortunately, when searching for matches in a dataset, the methods proposed in \cite{li2014deepreid}, \cite{ahmed2015improved} and \cite{chen2015deep} need to process pairs that include the query and every image in the dataset, and hence cannot directly utilize fast retrieval methods based on Euclidean and other simple distances. Here we aim at the approach that can learn per-image descriptors and then compare them with cosine similarity measure. This justifies starting with the architecture proposed in  \cite{yi2014deep} and then modifying it by inserting new layers.

There are several new works reporting results that are better than ours \cite{LiuFQJY17, XiaoLOW16} where additional data and/or sophisticated pre-training schemes were used, whereas we train our model from scratch on each dataset (except for CUHK01, where CUHK03 was used for pre-training).

\textbf{Bilinear CNNs.} Bilinear convolutional networks (Bilinear CNNs), introduced in \cite{lin2015bilinear} achieved state-of-the-art results for a number of fine-grained recognition tasks, and have also shown potential for face verification \cite{roychowdhury2015face}. Bilinear CNNs consists of two CNNs (where the input of these two CNNs is the same image) without fully-connected layers. The outputs of these two streams are combined in a special way via bilinear pooling. In more detail, the outer product of deep features are calculated for each spatial location, resulting in the quadratic number of feature maps, to which sum pooling over all locations is then performed. The resulting orderless image descriptor is then used in subsequent processing steps. For example, in \cite{lin2015bilinear} and \cite{roychowdhury2015face} it is normalized and fed into the softmax layer for classification. An intuition given in \cite{lin2015bilinear} is that the two CNN streams combined by bilinear operation may correspond to part and texture detectors respectively. This separation may facilitate localization when significant pose variation is present without the need for any part labeling of the training images. Our approach evaluates bilinear CNNs for the person re-identification tasks and improves this architecture by suggesting its multi-region variant.

